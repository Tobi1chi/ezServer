import os
import subprocess
import time
import json
import requests
import atexit
import signal
from typing import Optional

# ========== é…ç½® ==========
CONFIG = {
    "OLLAMA_DIR": r"D:\ollama\ollama-windows-amd64",
    "OLLAMA_HOME": r"D:\ollama\ollama-data",
    "OLLAMA_GPU_MEM": "2048",
    "OLLAMA_PORT": "11434",
    "OLLAMA_URL": "http://127.0.0.1:11434",
}

MODEL_MAP = {
    "phi3": "phi3",
    "llama": "llama3.2:3b",
    "qwen": "qwen2.5:7b",
}

OLLAMA_EXE = os.path.join(CONFIG["OLLAMA_DIR"], "ollama.exe")

# å…¨å±€è¿›ç¨‹å¼•ç”¨ï¼Œç”¨äºæ¸…ç†
_ollama_process: Optional[subprocess.Popen] = None


# ========== è¿›ç¨‹ç®¡ç† ==========

def cleanup_process():
    """æ¸…ç† Ollama è¿›ç¨‹"""
    global _ollama_process
    if _ollama_process and _ollama_process.poll() is None:
        print("\nğŸ›‘ æ­£åœ¨å…³é—­ Ollama æœåŠ¡...")
        _ollama_process.terminate()
        try:
            _ollama_process.wait(timeout=5)
            print("âœ… Ollama æœåŠ¡å·²å…³é—­")
        except subprocess.TimeoutExpired:
            print("âš ï¸ å¼ºåˆ¶ç»ˆæ­¢ Ollama æœåŠ¡")
            _ollama_process.kill()


def start_ollama_server():
    """å¯åŠ¨ Ollama æœåŠ¡"""
    global _ollama_process
    
    env = os.environ.copy()
    env["OLLAMA_HOME"] = CONFIG["OLLAMA_HOME"]
    env["OLLAMA_GPU_MEM"] = CONFIG["OLLAMA_GPU_MEM"]
    env["OLLAMA_PORT"] = CONFIG["OLLAMA_PORT"]

    print("ğŸš€ æ­£åœ¨å¯åŠ¨ Ollama æœåŠ¡...")
    _ollama_process = subprocess.Popen(
        [OLLAMA_EXE, "serve"],
        cwd=CONFIG["OLLAMA_DIR"],
        env=env,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    
    # æ³¨å†Œæ¸…ç†å‡½æ•°
    atexit.register(cleanup_process)
    signal.signal(signal.SIGINT, lambda s, f: (cleanup_process(), exit(0)))
    
    return _ollama_process, env


def wait_for_ollama_ready(timeout=60):
    """ç­‰å¾… Ollama æœåŠ¡å°±ç»ª"""
    url = f"{CONFIG['OLLAMA_URL']}/api/tags"
    t0 = time.time()
    
    while time.time() - t0 < timeout:
        try:
            r = requests.get(url, timeout=2)
            if r.status_code == 200:
                print("âœ… Ollama æœåŠ¡å·²å°±ç»ª")
                return True
        except Exception:
            pass
        print("â³ ç­‰å¾… Ollama å¯åŠ¨ä¸­...")
        time.sleep(2)
    
    print("âŒ ç­‰å¾… Ollama å¯åŠ¨è¶…æ—¶")
    return False


# ========== æ¨¡å‹ç®¡ç† ==========

def check_model_exists(env, model_name: str) -> bool:
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»ä¸‹è½½"""
    try:
        result = subprocess.run(
            [OLLAMA_EXE, "list"],
            cwd=CONFIG["OLLAMA_DIR"],
            env=env,
            capture_output=True,
            text=True,
            check=True,
        )
        # æ£€æŸ¥æ¨¡å‹åæ˜¯å¦åœ¨è¾“å‡ºä¸­
        return model_name in result.stdout
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥æ¨¡å‹æ—¶å‡ºé”™: {e}")
        return False


def pull_model(env, model_name: str):
    """æ‹‰å–æ¨¡å‹"""
    print(f"ğŸ”» å¼€å§‹æ‹‰å–æ¨¡å‹: {model_name}")
    try:
        subprocess.run(
            [OLLAMA_EXE, "pull", model_name],
            cwd=CONFIG["OLLAMA_DIR"],
            env=env,
            check=True,
        )
        print("âœ… æ¨¡å‹æ‹‰å–å®Œæˆ")
    except subprocess.CalledProcessError as e:
        print(f"âŒ æ¨¡å‹æ‹‰å–å¤±è´¥: {e}")
        raise


def ensure_model(env, model_name: str):
    """ç¡®ä¿æ¨¡å‹å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™æ‹‰å–"""
    if check_model_exists(env, model_name):
        print(f"âœ… æ¨¡å‹ {model_name} å·²å­˜åœ¨")
        return
    pull_model(env, model_name)


# ========== å¯¹è¯åŠŸèƒ½ ==========

def chat_stream(model_name: str, messages: list[dict]):
    """
    ä½¿ç”¨ /api/chat è¿›è¡Œæµå¼å¯¹è¯
    messages: [{"role": "system"/"user"/"assistant", "content": "..."}]
    """
    url = f"{CONFIG['OLLAMA_URL']}/api/chat"
    payload = {
        "model": model_name,
        "messages": messages,
        "stream": True,
    }

    with requests.post(url, json=payload, stream=True, timeout=300) as resp:
        resp.raise_for_status()
        for line in resp.iter_lines():
            if not line:
                continue
            data = json.loads(line.decode("utf-8"))
            msg = data.get("message", {})
            chunk = msg.get("content", "")
            if chunk:
                yield chunk
            if data.get("done"):
                break


def save_conversation(messages: list[dict], model_name: str):
    """ä¿å­˜å¯¹è¯å†å²"""
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    filename = f"chat_{model_name}_{timestamp}.json"
    
    try:
        with open(filename, "w", encoding="utf-8") as f:
            json.dump({
                "model": model_name,
                "timestamp": timestamp,
                "messages": messages
            }, f, ensure_ascii=False, indent=2)
        print(f"âœ… å¯¹è¯å·²ä¿å­˜è‡³: {filename}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")


def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    print("\nğŸ“– å‘½ä»¤åˆ—è¡¨:")
    print("  /exit   - é€€å‡ºå¯¹è¯")
    print("  /clear  - æ¸…ç©ºå¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆä¿ç•™ç³»ç»Ÿæç¤ºï¼‰")
    print("  /save   - ä¿å­˜å¯¹è¯å†å²åˆ° JSON æ–‡ä»¶")
    print("  /count  - æ˜¾ç¤ºå½“å‰å¯¹è¯è½®æ•°")
    print("  /help   - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")


def interactive_chat(model_name: str):
    """
    äº¤äº’å¼å¤šè½®å¯¹è¯
    """
    messages: list[dict] = [
        {
            "role": "system",
            "content": (
                "ä½ æ˜¯ä¸€ä¸ªå¸®æˆ‘å¤„ç†æœ¬åœ°é¡¹ç›®ã€æ•°æ®åº“å’Œæ¸¸æˆæœåŠ¡å™¨ç›¸å…³é—®é¢˜çš„ä¸­æ–‡åŠ©æ‰‹ï¼Œ"
                "å›ç­”å°½é‡ç®€æ´æ¸…æ™°ã€‚"
            )
        }
    ]

    print(f"\nğŸ’¬ å·²è¿›å…¥å¯¹è¯æ¨¡å¼ï¼Œå½“å‰æ¨¡å‹: {model_name}")
    show_help()

    while True:
        try:
            user_input = input("\nğŸ‘¤ ä½ : ").strip()
            
            if not user_input:
                continue
            
            # å¤„ç†å‘½ä»¤
            if user_input in {"/exit", "exit", "quit"}:
                print("ğŸ‘‹ ç»“æŸå¯¹è¯")
                break
            
            elif user_input == "/clear":
                messages = messages[:1]  # åªä¿ç•™ system æ¶ˆæ¯
                print("âœ… å·²æ¸…ç©ºå¯¹è¯ä¸Šä¸‹æ–‡")
                continue
            
            elif user_input == "/save":
                save_conversation(messages, model_name)
                continue
            
            elif user_input == "/count":
                # è®¡ç®—å¯¹è¯è½®æ•°ï¼ˆæ’é™¤ system æ¶ˆæ¯ï¼‰
                rounds = (len(messages) - 1) // 2
                print(f"ğŸ“Š å½“å‰å¯¹è¯è½®æ•°: {rounds}")
                continue
            
            elif user_input == "/help":
                show_help()
                continue
            
            # æ­£å¸¸å¯¹è¯æµç¨‹
            messages.append({"role": "user", "content": user_input})
            
            print("ğŸ¤– æ¨¡å‹: ", end="", flush=True)
            assistant_reply = ""
            
            try:
                for chunk in chat_stream(model_name, messages):
                    print(chunk, end="", flush=True)
                    assistant_reply += chunk
                
                print()  # æ¢è¡Œ
                
                if assistant_reply:
                    messages.append({"role": "assistant", "content": assistant_reply})
                else:
                    print("âš ï¸ æ¨¡å‹æœªè¿”å›å†…å®¹")
                    messages.pop()  # ç§»é™¤ç”¨æˆ·æ¶ˆæ¯
                    
            except requests.exceptions.RequestException as e:
                print(f"\nâŒ è¯·æ±‚å‡ºé”™: {e}")
                messages.pop()  # ç§»é™¤ç”¨æˆ·æ¶ˆæ¯
            except Exception as e:
                print(f"\nâŒ æœªçŸ¥é”™è¯¯: {e}")
                messages.pop()
                
        except KeyboardInterrupt:
            print("\nâš ï¸ æ£€æµ‹åˆ°ä¸­æ–­ï¼Œè¾“å…¥ /exit é€€å‡ºå¯¹è¯")
            continue
        except EOFError:
            print("\nğŸ‘‹ æ£€æµ‹åˆ°è¾“å…¥ç»“æŸï¼Œé€€å‡ºå¯¹è¯")
            break


# ========== æ¨¡å‹é€‰æ‹© ==========

def choose_model() -> str:
    """é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹"""
    print("\nğŸ“¦ è¯·é€‰æ‹©æ¨¡å‹:")
    print("  1) phi3   ->", MODEL_MAP["phi3"])
    print("  2) llama  ->", MODEL_MAP["llama"])
    print("  3) qwen   ->", MODEL_MAP["qwen"])

    choice = input("è¾“å…¥ 1 / 2 / 3 (é»˜è®¤ 1): ").strip()

    model_key = {
        "1": "phi3",
        "2": "llama",
        "3": "qwen",
    }.get(choice, "phi3")

    model_name = MODEL_MAP[model_key]
    print(f"âœ… å·²é€‰æ‹©æ¨¡å‹: {model_key} ({model_name})")
    return model_name


# ========== ä¸»å…¥å£ ==========

def main():
    """ä¸»å‡½æ•°"""
    try:
        # å¯åŠ¨ Ollama æœåŠ¡
        proc, env = start_ollama_server()
        
        # ç­‰å¾…æœåŠ¡å°±ç»ª
        if not wait_for_ollama_ready():
            print("âŒ Ollama å¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
            return 1
        
        # é€‰æ‹©æ¨¡å‹
        model_name = choose_model()
        
        # ç¡®ä¿æ¨¡å‹å­˜åœ¨
        ensure_model(env, model_name)
        
        # è¿›å…¥å¯¹è¯æ¨¡å¼
        interactive_chat(model_name)
        
        return 0
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç¨‹åºè¢«ä¸­æ–­")
        return 130
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1
    finally:
        cleanup_process()


if __name__ == "__main__":
    exit(main())